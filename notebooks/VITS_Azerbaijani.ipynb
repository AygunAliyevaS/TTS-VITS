{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# üéôÔ∏è VITS Azerbaijani Text-to-Speech\n",
        "\n",
        "This notebook provides a complete workflow for training and using a VITS model for Azerbaijani text-to-speech synthesis, including voice cloning capabilities.\n",
        "\n",
        "## üìã Features\n",
        "- Single-speaker TTS training\n",
        "- Zero-shot voice cloning\n",
        "- Audio preprocessing & normalization\n",
        "- Checkpoint management\n",
        "- Interactive Gradio demo\n",
        "\n",
        "## üó∫Ô∏è Notebook Structure\n",
        "1. Environment Setup\n",
        "2. Dataset Preparation\n",
        "3. Audio Preprocessing\n",
        "4. Model Training\n",
        "5. Inference & Voice Cloning\n",
        "6. Web Demo\n",
        "\n",
        "> üí° This notebook works both in Google Colab and locally. Colab-specific cells are marked with a [COLAB] tag.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "## 1. Environment Setup\n",
        "\n",
        "First, let's set up our environment with all necessary dependencies.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# [COLAB] System packages\n",
        "!apt-get update -y && apt-get install -y espeak ffmpeg\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install Python packages\n",
        "!pip install -q torch torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install -q numpy scipy librosa unidecode tensorboard phonemizer webdataset gradio tqdm pydub\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# [COLAB] Keep-alive function to prevent disconnects\n",
        "from IPython.display import display, Javascript\n",
        "\n",
        "def keep_alive():\n",
        "    display(Javascript('''\n",
        "        function ClickConnect(){\n",
        "            console.log(\"Clicking connect button...\");\n",
        "            document.querySelector(\"colab-connect-button\").click()\n",
        "        }\n",
        "        setInterval(ClickConnect, 60000)\n",
        "    '''))\n",
        "\n",
        "# Uncomment next line if running in Colab:\n",
        "# keep_alive()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 2. Dataset Preparation\n",
        "\n",
        "The VITS model requires:\n",
        "1. WAV audio files (22050 Hz, mono)\n",
        "2. Text transcriptions in Azerbaijani\n",
        "3. Filelists mapping audio to text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset upload helper\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "def upload_and_extract_dataset():\n",
        "    \"\"\"Upload and extract a dataset ZIP file to the datasets/raw directory.\"\"\"\n",
        "    print(\"Please upload your dataset ZIP file...\")\n",
        "    uploaded = files.upload()\n",
        "    \n",
        "    for filename in uploaded.keys():\n",
        "        if filename.endswith('.zip'):\n",
        "            print(f\"Extracting {filename} to datasets/raw/...\")\n",
        "            os.makedirs('datasets/raw', exist_ok=True)\n",
        "            !unzip -o \"{filename}\" -d datasets/raw/\n",
        "            print(\"Dataset extracted! Contents:\")\n",
        "            !ls -la datasets/raw/\n",
        "        else:\n",
        "            print(f\"Skipping {filename} - not a ZIP file\")\n",
        "\n",
        "# Uncomment to upload dataset:\n",
        "# upload_and_extract_dataset()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 3. Audio Preprocessing\n",
        "\n",
        "Before training, we'll normalize the audio files to ensure consistent quality:\n",
        "- Remove DC offset\n",
        "- Normalize levels\n",
        "- Resample to 22050 Hz\n",
        "- Convert to mono\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "import multiprocessing\n",
        "\n",
        "def process_audio_file(file_path, target_sr=22050, target_level=-23.0, output_dir=None):\n",
        "    \"\"\"Process a single audio file with normalization and resampling.\"\"\"\n",
        "    try:\n",
        "        # Set output path\n",
        "        if output_dir:\n",
        "            os.makedirs(output_dir, exist_ok=True)\n",
        "            filename = os.path.basename(file_path)\n",
        "            output_path = os.path.join(output_dir, filename)\n",
        "        else:\n",
        "            output_path = file_path\n",
        "            \n",
        "        # Load and process audio\n",
        "        y, sr = librosa.load(file_path, sr=None, mono=True)\n",
        "        \n",
        "        # Resample if needed\n",
        "        if sr != target_sr:\n",
        "            y = librosa.resample(y, orig_sr=sr, target_sr=target_sr)\n",
        "        \n",
        "        # Remove DC offset\n",
        "        y = y - np.mean(y)\n",
        "        \n",
        "        # Normalize level\n",
        "        rms = np.sqrt(np.mean(y**2))\n",
        "        target_rms = 10**(target_level/20)\n",
        "        gain = target_rms / (rms + 1e-8)\n",
        "        y_normalized = y * gain\n",
        "        \n",
        "        # Prevent clipping\n",
        "        max_val = np.max(np.abs(y_normalized))\n",
        "        if max_val > 0.99:\n",
        "            y_normalized = y_normalized / max_val * 0.99\n",
        "        \n",
        "        # Save processed audio\n",
        "        sf.write(output_path, y_normalized, target_sr)\n",
        "        return True\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {file_path}: {e}\")\n",
        "        return False\n",
        "\n",
        "def normalize_dataset(dataset_dir, output_dir=None):\n",
        "    \"\"\"Normalize all WAV files in a directory using multiprocessing.\"\"\"\n",
        "    wav_files = glob.glob(os.path.join(dataset_dir, \"**\", \"*.wav\"), recursive=True)\n",
        "    print(f\"Found {len(wav_files)} WAV files\")\n",
        "    \n",
        "    if not wav_files:\n",
        "        print(\"No WAV files found!\")\n",
        "        return\n",
        "    \n",
        "    # Process files in parallel\n",
        "    with multiprocessing.Pool(processes=os.cpu_count()) as pool:\n",
        "        args = [(f, 22050, -23.0, output_dir) for f in wav_files]\n",
        "        results = list(tqdm(pool.starmap(process_audio_file, args), total=len(args)))\n",
        "    \n",
        "    success_count = results.count(True)\n",
        "    print(f\"Successfully processed {success_count} of {len(wav_files)} files\")\n",
        "\n",
        "# Uncomment to normalize dataset:\n",
        "# normalize_dataset('datasets/raw', output_dir='datasets/normalized')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Generate Filelists\n",
        "\n",
        "Create train/validation splits with the format:\n",
        "```\n",
        "path/to/audio.wav|Azerbaijani text\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate train/val splits\n",
        "!python data/tools/prepare_filelist.py \\\n",
        "    --wavs datasets/raw \\\n",
        "    --output data/filelists \\\n",
        "    --val-ratio 0.05\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 4. Model Training\n",
        "\n",
        "We'll train the VITS model with periodic checkpointing and monitoring.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training monitor\n",
        "import time\n",
        "import os\n",
        "import glob\n",
        "\n",
        "def monitor_training(interval=60):\n",
        "    \"\"\"Monitor training progress and checkpoint saving.\"\"\"\n",
        "    checkpoint_dir = 'checkpoints'\n",
        "    \n",
        "    try:\n",
        "        while True:\n",
        "            checkpoint_files = glob.glob(f\"{checkpoint_dir}/*.pt\")\n",
        "            \n",
        "            print(f\"\\n=== Training Status: {time.strftime('%Y-%m-%d %H:%M:%S')} ===\")\n",
        "            print(f\"Found {len(checkpoint_files)} checkpoints\")\n",
        "            \n",
        "            if checkpoint_files:\n",
        "                checkpoint_files.sort(key=lambda x: os.path.getmtime(x), reverse=True)\n",
        "                print(\"\\nMost recent checkpoints:\")\n",
        "                for i, ckpt in enumerate(checkpoint_files[:3]):\n",
        "                    mod_time = time.strftime('%Y-%m-%d %H:%M:%S', \n",
        "                                        time.localtime(os.path.getmtime(ckpt)))\n",
        "                    size_mb = os.path.getsize(ckpt) / (1024 * 1024)\n",
        "                    print(f\"{i+1}. {os.path.basename(ckpt)} - {size_mb:.2f} MB - {mod_time}\")\n",
        "            \n",
        "            print(f\"\\nNext check in {interval} seconds...\")\n",
        "            time.sleep(interval)\n",
        "            \n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nMonitoring stopped\")\n",
        "\n",
        "# Start training\n",
        "!python train.py \\\n",
        "    --config config/base_vits.json \\\n",
        "    --batch_size 16 \\\n",
        "    --epochs 1000 \\\n",
        "    --checkpoint_dir checkpoints \\\n",
        "    --log_dir logs \\\n",
        "    --save_every 10 \\\n",
        "    --keep_last 3\n",
        "\n",
        "# Uncomment to monitor training:\n",
        "# monitor_training(interval=60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 5. Inference & Voice Cloning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import IPython.display as ipd\n",
        "from model.vits import VITSInference\n",
        "\n",
        "# Load model (uses best.pt or latest checkpoint)\n",
        "checkpoint_dir = 'checkpoints'\n",
        "checkpoint_files = glob.glob(f\"{checkpoint_dir}/*.pt\")\n",
        "\n",
        "if not checkpoint_files:\n",
        "    print(f\"No checkpoints found in {checkpoint_dir}!\")\n",
        "else:\n",
        "    # Prefer best.pt, fallback to latest\n",
        "    best_model = os.path.join(checkpoint_dir, 'best.pt')\n",
        "    if os.path.exists(best_model):\n",
        "        checkpoint_path = best_model\n",
        "    else:\n",
        "        checkpoint_path = max(checkpoint_files, key=os.path.getmtime)\n",
        "    \n",
        "    print(f\"Using checkpoint: {os.path.basename(checkpoint_path)}\")\n",
        "    \n",
        "    # Initialize model\n",
        "    tts = VITSInference(\n",
        "        checkpoint=checkpoint_path,\n",
        "        config='configs/base_vits.json')\n",
        "    \n",
        "    # Basic synthesis\n",
        "    text = \"Salam d√ºnya! Bu VITS n√ºmun…ôsidir.\"\n",
        "    audio = tts.synthesize(text)\n",
        "    ipd.display(ipd.Audio(audio, rate=22050))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Voice cloning demo\n",
        "def clone_voice(reference_wav=None):\n",
        "    \"\"\"Demonstrate voice cloning with a reference audio file.\"\"\"\n",
        "    if reference_wav is None:\n",
        "        # Upload reference voice\n",
        "        if 'google.colab' in globals():\n",
        "            print(\"Upload a reference voice file (.wav):\")\n",
        "            uploaded = files.upload()\n",
        "            if uploaded:\n",
        "                reference_wav = list(uploaded.keys())[0]\n",
        "        else:\n",
        "            # Use example file\n",
        "            reference_wav = 'datasets/raw/02.wav'\n",
        "    \n",
        "    if reference_wav and os.path.exists(reference_wav):\n",
        "        # Original reference audio\n",
        "        y, sr = librosa.load(reference_wav, sr=22050)\n",
        "        print(\"Reference voice:\")\n",
        "        ipd.display(ipd.Audio(y, rate=sr))\n",
        "        \n",
        "        # Cloned speech\n",
        "        text = \"M…ônim s…ôsiml…ô danƒ±≈üan s√ºni z…ôka!\"\n",
        "        print(f\"\\nCloned voice saying: {text}\")\n",
        "        cloned = tts.synthesize(text, speaker_ref=reference_wav)\n",
        "        ipd.display(ipd.Audio(cloned, rate=22050))\n",
        "\n",
        "# Try voice cloning\n",
        "if 'tts' in locals():\n",
        "    clone_voice()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 6. Web Demo\n",
        "\n",
        "Launch an interactive Gradio demo for testing the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Launch Gradio demo\n",
        "if 'google.colab' in globals():\n",
        "    !python app.py --share  # Public URL\n",
        "else:\n",
        "    !python app.py  # Local URL\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
