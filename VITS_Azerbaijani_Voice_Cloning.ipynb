{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VITS for Azerbaijani Voice Cloning and Audiobooks\n",
    "\n",
    "This notebook is enhanced for:\n",
    "1. **Azerbaijani Language:** Uses `espeak` for phonemization.\n",
    "2. **Large Datasets:** Implements `WebDataset` for efficient data loading, suitable for audiobook creation.\n",
    "3. **Voice Cloning:** Integrates a speaker encoder for zero-shot voice cloning from a reference audio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "We will install `espeak` for phonemization, `webdataset` for efficient data loading, and other necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install system-level dependency for phonemization\n",
    "!sudo apt-get update && sudo apt-get install -y espeak\n",
    "\n",
    "# Install Python libraries\n",
    "!pip install torch torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install numpy scipy librosa unidecode tensorboard\n",
    "!pip install phonemizer webdataset\n",
    "!pip install speechbrain\n",
    "\n",
    "# Clone VITS repository for base model and utilities\n",
    "!git clone https://github.com/jaywalnut310/vits.git\n",
    "%cd vits\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports, Configuration, and Azerbaijani Symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import webdataset as wds\n",
    "from speechbrain.pretrained import EncoderClassifier\n",
    "import librosa\n",
    "import IPython.display as ipd\n",
    "\n",
    "# Define Azerbaijani characters\n",
    "# Based on common Azerbaijani alphabet, including special characters\n",
    "_pad        = '_',\n",
    "_punctuation = '¡!¿?.,;:'\n",
    "_special    = ' -'\n",
    "_letters    = 'AaBbCcÇçDdEeƏəFfGgĞğHhXxIıİiJjKkQqLlMmNnOoÖöPpRrSsŞşTtUuÜüVvYyZz'\n",
    "\n",
    "# Export symbols\n",
    "symbols = list(_pad) + list(_special) + list(_punctuation) + list(_letters)\n",
    "\n",
    "# Create a mapping from symbol to numeric ID\n",
    "symbol_to_id = {s: i for i, s in enumerate(symbols)}\n",
    "\n",
    "# Configuration for the model\n",
    "config_json = {\n",
    "    \"train\": {\n",
    "        \"log_interval\": 200,\n",
    "        \"eval_interval\": 1000,\n",
    "        \"seed\": 1234,\n",
    "        \"epochs\": 20000,\n",
    "        \"learning_rate\": 2e-4,\n",
    "        \"betas\": [0.8, 0.99],\n",
    "        \"eps\": 1e-9,\n",
    "        \"batch_size\": 16,\n",
    "        \"fp16_run\": true,\n",
    "        \"lr_decay\": 0.999875,\n",
    "        \"segment_size\": 8192,\n",
    "        \"init_lr_ratio\": 1,\n",
    "        \"warmup_epochs\": 0,\n",
    "        \"c_mel\": 45,\n",
    "        \"c_kl\": 1.0\n",
    "    },\n",
    "    \"data\": {\n",
    "        \"webdataset_base_path\": \"./dataset_tar/az_audio-{000000..000099}.tar\",\n",
    "        \"text_cleaners\": [\"transliteration_cleaners\"],\n",
    "        \"language\": \"az\",\n",
    "        \"phonemizer\": \"espeak\",\n",
    "        \"max_wav_value\": 32768.0,\n",
    "        \"sampling_rate\": 22050,\n",
    "        \"filter_length\": 1024,\n",
    "        \"hop_length\": 256,\n",
    "        \"win_length\": 1024,\n",
    "        \"n_mel_channels\": 80,\n",
    "        \"add_blank\": true,\n",
    "        \"n_speakers\": 0, \"# Set to 0 for speaker embeddings\"\n",
    "        \"cleaned_text\": true\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"inter_channels\": 192,\n",
    "        \"hidden_channels\": 192,\n",
    "        \"filter_channels\": 768,\n",
    "        \"n_heads\": 2,\n",
    "        \"n_layers\": 6,\n",
    "        \"kernel_size\": 3,\n",
    "        \"p_dropout\": 0.1,\n",
    "        \"resblock\": \"1\",\n",
    "        \"resblock_kernel_sizes\": [3, 7, 11],\n",
    "        \"resblock_dilation_sizes\": [[1, 3, 5], [1, 3, 5], [1, 3, 5]],\n",
    "        \"upsample_rates\": [8, 8, 2, 2],\n",
    "        \"upsample_initial_channel\": 512,\n",
    "        \"upsample_kernel_sizes\": [16, 16, 4, 4],\n",
    "        \"n_layers_q\": 3,\n",
    "        \"use_spectral_norm\": false,\n",
    "        \"g_speaker_cond\": 512 \"# Dimension of speaker embedding\"\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('config_az.json', 'w') as f:\n",
    "    json.dump(config_json, f, indent=2)\n",
    "\n",
    "hps = utils.get_hparams_from_file(\"config_az.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation for Large Datasets\n",
    "\n",
    "For large datasets, we'll use `WebDataset`. This requires converting your dataset into a series of `.tar` files (shards). Each sample in the tar file will contain the audio and its transcript.\n",
    "\n",
    "**Action Required:** Create a filelist named `my_dataset.txt` with the format `path/to/audio.wav|text transcription`. Then run the cell below to create the tar shards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "filelist_path = 'my_dataset.txt' # Change this to your filelist\n",
    "output_dir = 'dataset_tar'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Read filelist\n",
    "with open(filelist_path, 'r', encoding='utf-8') as f:\n",
    "    filepaths_and_text = [line.strip().split('|') for line in f] \n",
    "\n",
    "# Create tar shards\n",
    "shard_size = 1000  # Number of samples per shard\n",
    "shard_count = 0\n",
    "for i in range(0, len(filepaths_and_text), shard_size):\n",
    "    shard_name = os.path.join(output_dir, f'az_audio-{shard_count:06d}.tar')\n",
    "    with tarfile.open(shard_name, 'w') as tar:\n",
    "        for wav_path, text in filepaths_and_text[i:i+shard_size]:\n",
    "            sample_name = os.path.splitext(os.path.basename(wav_path))[0]\n",
    "            \n",
    "            # Add audio file\n",
    "            tar.add(wav_path, arcname=f'{sample_name}.wav')\n",
    "            \n",
    "            # Add text file\n",
    "            text_bytes = text.encode('utf-8')\n",
    "            tarinfo = tarfile.TarInfo(name=f'{sample_name}.txt')\n",
    "            tarinfo.size = len(text_bytes)\n",
    "            tar.addfile(tarinfo, io.BytesIO(text_bytes))\n",
    "    print(f'Created shard: {shard_name}')\n",
    "    shard_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training with Voice Cloning\n",
    "\n",
    "We'll modify the training process to include speaker embeddings for voice cloning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: The standard VITS model needs to be modified to accept speaker embeddings.\n",
    "# The following is a conceptual guide. You would need to modify the SynthesizerTrn model in vits/models.py\n",
    "# to accept a `g_speaker` argument and use it in the generator.\n",
    "\n",
    "print('This section is a conceptual guide.')\n",
    "print('You need to modify the VITS model to accept speaker embeddings.')\n",
    "\n",
    "# Example of how the dataloader would look\n",
    "# This part is illustrative and assumes you have a function to process the data\n",
    "# url = hps.data.webdataset_base_path\n",
    "# dataset = wds.Dataset(url).shuffle(1000).decode('torch_audio').to_tuple('wav', 'txt')\n",
    "# train_loader = DataLoader(dataset, batch_size=hps.train.batch_size, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Inference with Voice Cloning\n",
    "\n",
    "For inference, we'll load a reference audio, generate its speaker embedding, and use it to synthesize speech in the target voice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained speaker encoder\n",
    "speaker_encoder = EncoderClassifier.from_hparams(source=\"speechbrain/spkrec-ecapa-voxceleb\", savedir=\"pretrained_models/spkrec-ecapa-voxceleb\")\n",
    "\n",
    "def get_speaker_embedding(wav_path):\n",
    "    audio, sample_rate = librosa.load(wav_path, sr=16000)\n",
    "    audio_tensor = torch.FloatTensor(audio).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        embedding = speaker_encoder.encode_batch(audio_tensor)\n",
    "        embedding = embedding.squeeze(0).squeeze(0) # [1, 1, 192] -> [192]\n",
    "    return embedding.cpu().numpy()\n",
    "\n",
    "# Path to your reference audio for voice cloning\n",
    "reference_audio_path = 'path/to/your/reference.wav' # <<< CHANGE THIS\n",
    "speaker_embedding = get_speaker_embedding(reference_audio_path)\n",
    "\n",
    "# Load trained VITS model (conceptual)\n",
    "# net_g = SynthesizerTrn(...) \n",
    "# utils.load_checkpoint('path/to/your/model.pth', net_g, None)\n",
    "\n",
    "text_to_synthesize = \"Salam dünya, bu bir səs klonlama testidir.\"\n",
    "# text_sequence = text_to_sequence(text_to_synthesize, hps.data.text_cleaners)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     x_tst = ...\n",
    "#     speaker_emb_tst = torch.FloatTensor(speaker_embedding).unsqueeze(0).to(device)\n",
    "#     audio = net_g.infer(x_tst, ..., g=speaker_emb_tst) # 'g' is the speaker embedding\n",
    "\n",
    "print(f'Generated speaker embedding of shape: {speaker_embedding.shape}')\n",
    "print('Inference part is conceptual and requires a model trained with speaker embeddings.')\n",
    "# ipd.display(ipd.Audio(audio, rate=hps.data.sampling_rate))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
