{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VITS Training and Inference Notebook\n",
    "\n",
    "This notebook provides a complete workflow for training a VITS (Variational Inference with adversarial learning for end-to-end Text-to-Speech) model and using it for inference. It is designed to be run on Google Colab with a free GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "First, we install the necessary libraries. We'll also clone the original VITS repository to use some of their code for data processing and the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install torch torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install numpy scipy librosa unidecode tensorboard\n",
    "!pip install phonemizer\n",
    "\n",
    "# Clone VITS repository for utilities\n",
    "!git clone https://github.com/jaywalnut310/vits.git\n",
    "%cd vits\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import commons\n",
    "import utils\n",
    "from data_utils import (\n",
    "  TextAudioLoader,\n",
    "  TextAudioCollate,\n",
    "  DistributedBucketSampler\n",
    ")\n",
    "from models import (\n",
    "  SynthesizerTrn,\n",
    "  MultiPeriodDiscriminator,\n",
    ")\n",
    "from text.symbols import symbols\n",
    "from text import text_to_sequence\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import librosa\n",
    "import IPython.display as ipd\n",
    "\n",
    "# Check for GPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using {device} device')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a configuration file\n",
    "We'll create a JSON config file for our model. This is based on the `base_vits.json` you have and common parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_json = {\n",
    "    \"train\": {\n",
    "        \"log_interval\": 200,\n",
    "        \"eval_interval\": 1000,\n",
    "        \"seed\": 1234,\n",
    "        \"epochs\": 10000,\n",
    "        \"learning_rate\": 2e-4,\n",
    "        \"betas\": [0.8, 0.99],\n",
    "        \"eps\": 1e-9,\n",
    "        \"batch_size\": 16,\n",
    "        \"fp16_run\": true,\n",
    "        \"lr_decay\": 0.999875,\n",
    "        \"segment_size\": 8192,\n",
    "        \"init_lr_ratio\": 1,\n",
    "        \"warmup_epochs\": 0,\n",
    "        \"c_mel\": 45,\n",
    "        \"c_kl\": 1.0\n",
    "    },\n",
    "    \"data\": {\n",
    "        \"training_files\": \"filelists/ljs_audio_text_train_filelist.txt.cleaned\",\n",
    "        \"validation_files\": \"filelists/ljs_audio_text_val_filelist.txt.cleaned\",\n",
    "        \"text_cleaners\": [\"english_cleaners2\"],\n",
    "        \"max_wav_value\": 32768.0,\n",
    "        \"sampling_rate\": 22050,\n",
    "        \"filter_length\": 1024,\n",
    "        \"hop_length\": 256,\n",
    "        \"win_length\": 1024,\n",
    "        \"n_mel_channels\": 80,\n",
    "        \"mel_fmin\": 0.0,\n",
    "        \"mel_fmax\": null,\n",
    "        \"add_blank\": true,\n",
    "        \"n_speakers\": 0,\n",
    "        \"cleaned_text\": true\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"inter_channels\": 192,\n",
    "        \"hidden_channels\": 192,\n",
    "        \"filter_channels\": 768,\n",
    "        \"n_heads\": 2,\n",
    "        \"n_layers\": 6,\n",
    "        \"kernel_size\": 3,\n",
    "        \"p_dropout\": 0.1,\n",
    "        \"resblock\": \"1\",\n",
    "        \"resblock_kernel_sizes\": [3, 7, 11],\n",
    "        \"resblock_dilation_sizes\": [[1, 3, 5], [1, 3, 5], [1, 3, 5]],\n",
    "        \"upsample_rates\": [8, 8, 2, 2],\n",
    "        \"upsample_initial_channel\": 512,\n",
    "        \"upsample_kernel_sizes\": [16, 16, 4, 4],\n",
    "        \"n_layers_q\": 3,\n",
    "        \"use_spectral_norm\": false\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('config.json', 'w') as f:\n",
    "    json.dump(config_json, f, indent=2)\n",
    "\n",
    "hps = utils.get_hparams_from_file(\"config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation\n",
    "\n",
    "Here, you need to prepare your dataset. The VITS model expects a specific filelist format: `path/to/audio.wav|text transcription`.\n",
    "\n",
    "For this example, we'll download the LJSpeech dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and extract LJSpeech\n",
    "!wget https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\n",
    "!tar -xjvf LJSpeech-1.1.tar.bz2\n",
    "!mkdir -p filelists\n",
    "\n",
    "# Create filelists\n",
    "!wget https://raw.githubusercontent.com/NVIDIA/tacotron2/master/filelists/ljs_audio_text_train_filelist.txt\n",
    "!wget https://raw.githubusercontent.com/NVIDIA/tacotron2/master/filelists/ljs_audio_text_val_filelist.txt\n",
    "!wget https://raw.githubusercontent.com/NVIDIA/tacotron2/master/filelists/ljs_audio_text_test_filelist.txt\n",
    "\n",
    "# Clean the text\n",
    "!python preprocess.py --text_index 1 --filelists ljs_audio_text_train_filelist.txt ljs_audio_text_val_filelist.txt --text_cleaners english_cleaners2\n",
    "\n",
    "# Move filelists to the correct directory\n",
    "!mv ljs_audio_text_train_filelist.txt.cleaned filelists/\n",
    "!mv ljs_audio_text_val_filelist.txt.cleaned filelists/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training\n",
    "\n",
    "Now we will set up the training components and start the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(hps.train.seed)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Create logger\n",
    "logger = utils.get_logger(hps.model_dir)\n",
    "writer = SummaryWriter(log_dir=hps.model_dir)\n",
    "\n",
    "# Create dataset and dataloader\n",
    "train_dataset = TextAudioLoader(hps.data.training_files, hps.data)\n",
    "collate_fn = TextAudioCollate()\n",
    "train_loader = DataLoader(train_dataset, batch_size=hps.train.batch_size, num_workers=2, shuffle=True, pin_memory=True, collate_fn=collate_fn)\n",
    "\n",
    "# Create models\n",
    "net_g = SynthesizerTrn(\n",
    "    len(symbols),\n",
    "    hps.data.filter_length // 2 + 1,\n",
    "    hps.train.segment_size // hps.data.hop_length,\n",
    "    **hps.model).to(device)\n",
    "\n",
    "net_d = MultiPeriodDiscriminator(hps.model.use_spectral_norm).to(device)\n",
    "\n",
    "# Create optimizers\n",
    "optim_g = torch.optim.AdamW(net_g.parameters(), hps.train.learning_rate, betas=hps.train.betas, eps=hps.train.eps)\n",
    "optim_d = torch.optim.AdamW(net_d.parameters(), hps.train.learning_rate, betas=hps.train.betas, eps=hps.train.eps)\n",
    "\n",
    "# Schedulers\n",
    "scheduler_g = torch.optim.lr_scheduler.ExponentialLR(optim_g, gamma=hps.train.lr_decay)\n",
    "scheduler_d = torch.optim.lr_scheduler.ExponentialLR(optim_d, gamma=hps.train.lr_decay)\n",
    "\n",
    "# Scaler for mixed precision training\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=hps.train.fp16_run)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1, hps.train.epochs + 1):\n",
    "    net_g.train()\n",
    "    net_d.train()\n",
    "    for batch_idx, (x, x_lengths, spec, spec_lengths, y, y_lengths) in enumerate(train_loader):\n",
    "        x, x_lengths = x.to(device), x_lengths.to(device)\n",
    "        spec, spec_lengths = spec.to(device), spec_lengths.to(device)\n",
    "        y, y_lengths = y.to(device), y_lengths.to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=hps.train.fp16_run):\n",
    "            y_hat, l_length, attn, ids_slice, x_mask, z_mask, (z, z_p, m_p, logs_p, m_q, logs_q) = net_g(x, x_lengths, spec, spec_lengths)\n",
    "\n",
    "            mel = spec_to_mel_torch(\n",
    "                spec,\n",
    "                hps.data.filter_length,\n",
    "                hps.data.n_mel_channels,\n",
    "                hps.data.sampling_rate,\n",
    "                hps.data.mel_fmin,\n",
    "                hps.data.mel_fmax)\n",
    "            y_mel = commons.slice_segments(mel, ids_slice, hps.train.segment_size // hps.data.hop_length)\n",
    "            y_hat_mel = mel_spectrogram_torch(\n",
    "                y_hat.squeeze(1),\n",
    "                hps.data.filter_length,\n",
    "                hps.data.n_mel_channels,\n",
    "                hps.data.sampling_rate,\n",
    "                hps.data.hop_length,\n",
    "                hps.data.win_length,\n",
    "                hps.data.mel_fmin,\n",
    "                hps.data.mel_fmax\n",
    "            )\n",
    "\n",
    "            y = commons.slice_segments(y, ids_slice * hps.data.hop_length, hps.train.segment_size)\n",
    "\n",
    "            # Discriminator\n",
    "            y_d_hat_r, y_d_hat_g, _, _ = net_d(y, y_hat.detach())\n",
    "            with torch.cuda.amp.autocast(enabled=False):\n",
    "                loss_disc, losses_disc_r, losses_disc_g = discriminator_loss(y_d_hat_r, y_d_hat_g)\n",
    "                loss_disc_all = loss_disc\n",
    "\n",
    "        optim_d.zero_grad()\n",
    "        scaler.scale(loss_disc_all).backward()\n",
    "        scaler.unscale_(optim_d)\n",
    "        grad_norm_d = commons.clip_grad_value_(net_d.parameters(), None)\n",
    "        scaler.step(optim_d)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=hps.train.fp16_run):\n",
    "            # Generator\n",
    "            y_d_hat_r, y_d_hat_g, fmap_r, fmap_g = net_d(y, y_hat)\n",
    "            with torch.cuda.amp.autocast(enabled=False):\n",
    "                loss_mel = F.l1_loss(y_mel, y_hat_mel) * hps.train.c_mel\n",
    "                loss_kl = kl_loss(z_p, logs_q, m_p, logs_p, z_mask) * hps.train.c_kl\n",
    "                loss_fm = feature_loss(fmap_r, fmap_g)\n",
    "                loss_gen, losses_gen = generator_loss(y_d_hat_g)\n",
    "                loss_gen_all = loss_gen + loss_fm + loss_mel + loss_kl\n",
    "\n",
    "        optim_g.zero_grad()\n",
    "        scaler.scale(loss_gen_all).backward()\n",
    "        scaler.unscale_(optim_g)\n",
    "        grad_norm_g = commons.clip_grad_value_(net_g.parameters(), None)\n",
    "        scaler.step(optim_g)\n",
    "        scaler.update()\n",
    "\n",
    "        if (batch_idx % hps.train.log_interval == 0):\n",
    "            logger.info(f'Epoch: {epoch}, Batch: {batch_idx}, Gen_loss: {loss_gen_all}, Disc_loss: {loss_disc_all}')\n",
    "\n",
    "    scheduler_g.step()\n",
    "    scheduler_d.step()\n",
    "\n",
    "    if (epoch % hps.train.eval_interval == 0):\n",
    "        utils.save_checkpoint(net_g, optim_g, hps.train.learning_rate, epoch, os.path.join(hps.model_dir, \"G_{}.pth\".format(epoch)))\n",
    "        utils.save_checkpoint(net_d, optim_d, hps.train.learning_rate, epoch, os.path.join(hps.model_dir, \"D_{}.pth\".format(epoch)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Inference\n",
    "\n",
    "Once the model is trained, you can use it to synthesize speech from text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(text, hps):\n",
    "    text_norm = text_to_sequence(text, hps.data.text_cleaners)\n",
    "    if hps.data.add_blank:\n",
    "        text_norm = commons.intersperse(text_norm, 0)\n",
    "    text_norm = torch.LongTensor(text_norm)\n",
    "    return text_norm\n",
    "\n",
    "# Load the trained generator model\n",
    "# Make sure to replace G_XXXX.pth with your trained model checkpoint\n",
    "model_path = \"./logs/G_10000.pth\" # Change this to your model path\n",
    "hps_inf = utils.get_hparams_from_file(\"./config.json\")\n",
    "net_g_inf = SynthesizerTrn(\n",
    "    len(symbols),\n",
    "    hps_inf.data.filter_length // 2 + 1,\n",
    "    hps_inf.train.segment_size // hps_inf.data.hop_length,\n",
    "    **hps_inf.model).to(device)\n",
    "_ = net_g_inf.eval()\n",
    "_ = utils.load_checkpoint(model_path, net_g_inf, None)\n",
    "\n",
    "# Text to synthesize\n",
    "text = \"Hello world, this is a test of the VITS model.\"\n",
    "stn_tst = get_text(text, hps_inf)\n",
    "\n",
    "with torch.no_grad():\n",
    "    x_tst = stn_tst.to(device).unsqueeze(0)\n",
    "    x_tst_lengths = torch.LongTensor([stn_tst.size(0)]).to(device)\n",
    "    audio = net_g_inf.infer(x_tst, x_tst_lengths, noise_scale=.667, noise_scale_w=0.8, length_scale=1)[0][0,0].data.cpu().float().numpy()\n",
    "\n",
    "# Display and save the audio\n",
    "print(\"Generated Audio:\")\n",
    "ipd.display(ipd.Audio(audio, rate=hps_inf.data.sampling_rate, normalize=False))\n",
    "librosa.output.write_wav(\"./generated_audio.wav\", audio, hps_inf.data.sampling_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
